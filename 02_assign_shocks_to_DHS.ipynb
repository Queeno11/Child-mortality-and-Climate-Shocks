{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # for notebooks\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "tqdm.pandas()\n",
    "\n",
    "# Set global variables\n",
    "PROJECT = r\"Z:\\Laboral\\World Bank\\Paper - Child mortality and Climate Shocks\"\n",
    "OUTPUTS = rf\"{PROJECT}\\Outputs\"\n",
    "DATA = rf\"{PROJECT}\\Data\"\n",
    "DATA_IN = rf\"{DATA}\\Data_in\"\n",
    "DATA_PROC = rf\"{DATA}\\Data_proc\"\n",
    "DATA_OUT = rf\"{DATA}\\Data_out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to assign climate shocks from a date and a point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\backends\\plugins.py:80: RuntimeWarning: Engine 'cfgrib' loading failed:\n",
      "Cannot find the ecCodes library\n",
      "  warnings.warn(f\"Engine {name!r} loading failed:\\n{ex}\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "climate_data = xr.open_dataset(rf\"{DATA_OUT}/Climate_shocks_v2_previous_months.nc\")\n",
    "dates = climate_data.time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_climate_shock(from_date, to_date, lat, lon):\n",
    "    if pd.isna(from_date):\n",
    "        return np.nan\n",
    "    \n",
    "    # Filter point    \n",
    "    point_data = climate_data.sel(time=slice(from_date, to_date)).sel(lat=lat, lon=lon, method='nearest')\n",
    "    \n",
    "    # Get position of original data\n",
    "    lat = point_data.lat.item()\n",
    "    lon = point_data.lon.item()\n",
    "\n",
    "    # Filter by time\n",
    "    inutero_q1   = point_data.isel(time=slice(0,3))\n",
    "    inutero_q2   = point_data.isel(time=slice(3,6))\n",
    "    inutero_q3   = point_data.isel(time=slice(6,9))\n",
    "    born_1m      = point_data.isel(time=slice(9,10))\n",
    "    born_2to3m  = point_data.isel(time=slice(10,12))\n",
    "    born_3to6m  = point_data.isel(time=slice(12,15))\n",
    "    born_6to12m  = point_data.isel(time=slice(15,21))\n",
    "\n",
    "    out_vars = [lat, lon, ]\n",
    "    for prec in [\"standarized_precipitation\", \"standarized_precipitation_3\", \"standarized_precipitation_6\", \"standarized_precipitation_12\"]:\n",
    "        # Compute min and max values for both variables\n",
    "        inutero_q1_mean   = inutero_q1[prec].mean().item()\n",
    "        inutero_q2_mean   = inutero_q2[prec].mean().item()\n",
    "        inutero_q3_mean   = inutero_q3[prec].mean().item()\n",
    "        born_1m_mean      = born_1m[prec].mean().item()\n",
    "        born_2to3m_mean  = born_2to3m[prec].mean().item()\n",
    "        born_3to6m_mean  = born_3to6m[prec].mean().item()\n",
    "        born_6to12m_mean  = born_6to12m[prec].mean().item()\n",
    "\n",
    "        out_vars_this_prec = [inutero_q1_mean, inutero_q2_mean, inutero_q3_mean, born_1m_mean, born_2to3m_mean, born_3to6m_mean, born_6to12m_mean]\n",
    "        out_vars += out_vars_this_prec\n",
    "\n",
    "    return out_vars    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_climate_shock_old(from_date, to_date, lat, lon):\n",
    "    if pd.isna(from_date):\n",
    "        return np.nan\n",
    "    \n",
    "    # Filter point    \n",
    "    point_data = climate_data.sel(time=slice(from_date, to_date)).sel(lat=lat, lon=lon, method='nearest')\n",
    "    \n",
    "    # Get max and min values for standarized precipitation\n",
    "    max_prec = point_data[\"standarized_precipitation\"].max().item()\n",
    "    min_prec = point_data[\"standarized_precipitation\"].min().item()\n",
    "    max_prec_m = point_data[\"standarized_precipitation_m\"].max().item()\n",
    "    min_prec_m = point_data[\"standarized_precipitation_m\"].min().item()\n",
    "    \n",
    "    # Get position of original data\n",
    "    lat = point_data.lat.item()\n",
    "    lon = point_data.lon.item()\n",
    "    \n",
    "    return lat, lon, max_prec, min_prec, max_prec_m, min_prec_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04710157960653305"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_data.isel(time=slice(100, 110), lat=-50, lon=120)[\"standarized_precipitation\"].mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "def get_climate_shock_prof():\n",
    "    date = np.random.choice(dates[12:-12])\n",
    "    from_date, to_date = date + pd.DateOffset(months=-9), date + pd.DateOffset(years=1)\n",
    "    lat, lon = np.random.uniform(-90, 90), np.random.uniform(-180, 180)\n",
    "    \n",
    "    get_climate_shock(from_date, to_date, lat, lon)    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f get_climate_shock_prof get_climate_shock_prof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit get_climate_shock_prof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open DHS data and add the shock data to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_stata(rf\"{DATA_IN}/DHS/DHSBirthsGlobalAnalysis_04172024.dta\")\n",
    "df['ID'] = df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dates variables:\n",
    "We considered a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datetime object from year and month\n",
    "df[\"day\"] = 1\n",
    "df[\"month\"] = df[\"chb_month\"].astype(float)\n",
    "df[\"year\"] = df[\"chb_year\"].astype(float)\n",
    "df[\"birthdate\"] = pd.to_datetime(df[[\"year\", \"month\",\"day\"]]).to_numpy()\n",
    "\n",
    "# Maximum range of dates\n",
    "df[\"from_date\"] = df[\"birthdate\"] + pd.DateOffset(months=-9) # From in utero (9 months before birth) \n",
    "df[\"to_date\"] = df[\"birthdate\"] + pd.DateOffset(years=1) # To the first year of life\n",
    "\n",
    "# Filter children from_date greater than 1990 (we only have climate data from 1990)\n",
    "df = df[df[\"from_date\"] > \"1990-01-01\"]\n",
    "\n",
    "# # Construct deathdate variable\n",
    "# df[\"deathdate\"] = df[df[\"child_agedeath\"]<=12].progress_apply(lambda x: x[\"birthdate\"] + pd.DateOffset(months=x[\"child_agedeath\"]), axis=1)\n",
    "\n",
    "# # Replace to_date with deathdate if the child died\n",
    "# df[\"to_date\"] = np.where((df[\"child_agedeath\"]<=12) & (df[\"deathdate\"]<df[\"to_date\"]), df[\"deathdate\"], df[\"to_date\"])\n",
    "\n",
    "# Filter children to_date smalle than 2021 (we only have climate data to 2020)\n",
    "df = df[df[\"to_date\"] < \"2021-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/526 [27:39<38:09:59, 264.23s/it]"
     ]
    }
   ],
   "source": [
    "coords_cols = [\"lat_climate\", \"lon_climate\"]\n",
    "prec_cols = [\"prec_inutero_q1\", \"prec_inutero_q2\", \"prec_inutero_q3\", \"prec_born_1m\", \"prec_born_2to3m\", \"prec_born_3to6m\", \"prec_born_6to12m\"]\n",
    "prec_3_cols = [\"prec_3_inutero_q1\", \"prec_3_inutero_q2\", \"prec_3_inutero_q3\", \"prec_3_born_1m\", \"prec_3_born_2to3m\", \"prec_3_born_3to6m\", \"prec_3_born_6to12m\"]\n",
    "prec_6_cols = [\"prec_6_inutero_q1\", \"prec_6_inutero_q2\", \"prec_6_inutero_q3\", \"prec_6_born_1m\", \"prec_6_born_2to3m\", \"prec_6_born_3to6m\", \"prec_6_born_6to12m\"]\n",
    "prec_12_cols = [\"prec_12_inutero_q1\", \"prec_12_inutero_q2\", \"prec_12_inutero_q3\", \"prec_12_born_1m\", \"prec_12_born_2to3m\", \"prec_12_born_3to6m\", \"prec_12_born_6to12m\"]\n",
    "all_cols = coords_cols + prec_cols + prec_3_cols + prec_6_cols + prec_12_cols\n",
    "\n",
    "for n in tqdm(range(0, df.ID.max(), 10_000)):\n",
    "    if os.path.exists(rf\"{DATA_PROC}/births_climate_{n}.csv\"):\n",
    "        print(f\"births_climate_{n}.csv exists, moving to next iteration\")\n",
    "        continue\n",
    "    chunk = df.loc[(df.ID >= n) & (df.ID < n+10_000), ['ID', 'from_date', 'to_date', 'LATNUM', 'LONGNUM']].copy()\n",
    "    if chunk.shape[0]==0:\n",
    "        continue\n",
    "    climate_results = chunk[['from_date', 'to_date', 'LATNUM', 'LONGNUM']].apply(lambda s: get_climate_shock(s['from_date'], s['to_date'], s['LATNUM'], s['LONGNUM']), axis=1)\n",
    "    climate_results = climate_results.apply(pd.Series)\n",
    "    climate_results.columns = all_cols\n",
    "    climate_results[\"ID\"] = chunk[\"ID\"]\n",
    "    climate_results.to_csv(rf\"{DATA_PROC}/births_climate_{n}.csv\")\n",
    "    \n",
    "# df[all_cols] = climate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For debugging\n",
    "# chunk[['from_date', 'to_date', 'LATNUM', 'LONGNUM']].progress_apply(lambda s: get_climate_shock(s['from_date'], s['to_date'], s['LATNUM'], s['LONGNUM']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(rf\"{DATA_PROC}\") \n",
    "files = [f for f in files if f.startswith(\"births_climate_\")]\n",
    "data = []\n",
    "for file in tqdm(files):\n",
    "    df = pd.read_csv(rf\"{DATA_PROC}/{file}\")\n",
    "    data += [df]\n",
    "df = pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=\"Unnamed: 0\")\n",
    "df.to_stata(rf\"{DATA_PROC}\\ClimateShocks_assigned.dta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pos = np.random.randint(0, 500000)\n",
    "pos = 428380      \n",
    "filtered2 = climate_data.standarized_precipitation.sel(lat=df.at[pos, \"LATNUM\"], lon=df.at[pos, \"LONGNUM\"], method=\"nearest\")\n",
    "filtered2.plot(figsize=(12, 2))\n",
    "\n",
    "plt.axhline(2, color=\"red\")\n",
    "plt.axhline(-2, color=\"red\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v000</th>\n",
       "      <th>v001</th>\n",
       "      <th>v002</th>\n",
       "      <th>v003</th>\n",
       "      <th>v007</th>\n",
       "      <th>...</th>\n",
       "      <th>mother_ageb_squ</th>\n",
       "      <th>mother_ageb_cub</th>\n",
       "      <th>mother_eduy_squ</th>\n",
       "      <th>mother_eduy_cub</th>\n",
       "      <th>birth_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>...</td>\n",
       "      <td>560.111145</td>\n",
       "      <td>13255.964844</td>\n",
       "      <td>256.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>...</td>\n",
       "      <td>774.694397</td>\n",
       "      <td>21562.326172</td>\n",
       "      <td>256.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL5</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>...</td>\n",
       "      <td>423.673645</td>\n",
       "      <td>8720.616211</td>\n",
       "      <td>64.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AL5</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>...</td>\n",
       "      <td>604.340332</td>\n",
       "      <td>14856.699219</td>\n",
       "      <td>64.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL5</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>...</td>\n",
       "      <td>654.506897</td>\n",
       "      <td>16744.466797</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1728.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4780579</th>\n",
       "      <td>ZW7</td>\n",
       "      <td>99</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "      <td>...</td>\n",
       "      <td>348.444427</td>\n",
       "      <td>6504.295410</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4780580</th>\n",
       "      <td>ZW7</td>\n",
       "      <td>99</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>...</td>\n",
       "      <td>280.562500</td>\n",
       "      <td>4699.421875</td>\n",
       "      <td>64.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4780581</th>\n",
       "      <td>ZW7</td>\n",
       "      <td>99</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>...</td>\n",
       "      <td>280.562500</td>\n",
       "      <td>4699.421875</td>\n",
       "      <td>64.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4780582</th>\n",
       "      <td>ZW7</td>\n",
       "      <td>99</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>...</td>\n",
       "      <td>413.444458</td>\n",
       "      <td>8406.704102</td>\n",
       "      <td>64.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4780583</th>\n",
       "      <td>ZW7</td>\n",
       "      <td>99</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>...</td>\n",
       "      <td>413.444458</td>\n",
       "      <td>8406.704102</td>\n",
       "      <td>64.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4780584 rows × 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v000  v001  v002  v003  v007  ...  mother_ageb_squ mother_ageb_cub  \\\n",
       "0        AL5     1     1     3  2008  ...       560.111145    13255.964844   \n",
       "1        AL5     1     1     3  2008  ...       774.694397    21562.326172   \n",
       "2        AL5     1    10     4  2008  ...       423.673645     8720.616211   \n",
       "3        AL5     1    10     4  2008  ...       604.340332    14856.699219   \n",
       "4        AL5     1    12     3  2008  ...       654.506897    16744.466797   \n",
       "...      ...   ...   ...   ...   ...  ...              ...             ...   \n",
       "4780579  ZW7    99     8     3  2015  ...       348.444427     6504.295410   \n",
       "4780580  ZW7    99     9     1  2015  ...       280.562500     4699.421875   \n",
       "4780581  ZW7    99     9     1  2015  ...       280.562500     4699.421875   \n",
       "4780582  ZW7    99     9     1  2015  ...       413.444458     8406.704102   \n",
       "4780583  ZW7    99     9     1  2015  ...       413.444458     8406.704102   \n",
       "\n",
       "        mother_eduy_squ mother_eduy_cub birth_order  \n",
       "0                 256.0          4096.0         1.0  \n",
       "1                 256.0          4096.0         2.0  \n",
       "2                  64.0           512.0         1.0  \n",
       "3                  64.0           512.0         2.0  \n",
       "4                 144.0          1728.0         1.0  \n",
       "...                 ...             ...         ...  \n",
       "4780579           100.0          1000.0         2.0  \n",
       "4780580            64.0           512.0         1.0  \n",
       "4780581            64.0           512.0         2.0  \n",
       "4780582            64.0           512.0         3.0  \n",
       "4780583            64.0           512.0         4.0  \n",
       "\n",
       "[4780584 rows x 158 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_stata(rf\"{DATA_PROC}\\ClimateShocks_assigned.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_stata(rf\"{DATA_IN}/DHS/DHSBirthsGlobalAnalysis_04172024.dta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"ID\"] = df2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = df2.merge(df, on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ofici\\AppData\\Local\\Temp\\ipykernel_7324\\802629242.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged[\"ID_cell\"] = merged[\"lon_climate\"].astype(str) + \"_\" + merged[\"lat_climate\"].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# * Genero ID_cell con las celdas originales\n",
    "# tostring lon_climate lat_climate , generate(lon_climate_str lat_climate_str )\n",
    "# gen ID_cell_str = lat_climate_str + \"-\" + lon_climate_str\n",
    "# encode ID_cell_str, gen(ID_cell)\n",
    "# drop ID_cell_str lon_climate_str lat_climate_str \n",
    "\n",
    "merged[\"ID_cell\"] = merged[\"lon_climate\"].astype(str) + \"_\" + merged[\"lat_climate\"].astype(str)\n",
    "one_hot = pd.get_dummies(merged[\"ID_cell\"], prefix='ID_cell_')\n",
    "year_one_hot = one_hot * merged[\"chb_year\"]\n",
    "month_one_hot = one_hot * merged[\"chb_month\"]\n",
    "# merged = df.join(one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
