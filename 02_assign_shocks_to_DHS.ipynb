{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # for notebooks\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "tqdm.pandas()\n",
    "\n",
    "# Set global variables\n",
    "PROJECT = r\"Z:\\Laboral\\World Bank\\Paper - Child mortality and Climate Shocks\"\n",
    "OUTPUTS = rf\"{PROJECT}\\Outputs\"\n",
    "DATA = rf\"{PROJECT}\\Data\"\n",
    "DATA_IN = rf\"{DATA}\\Data_in\"\n",
    "DATA_PROC = rf\"{DATA}\\Data_proc\"\n",
    "DATA_OUT = rf\"{DATA}\\Data_out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to assign climate shocks from a date and a point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_data = xr.open_dataset(rf\"{DATA_OUT}/Climate_shocks_v2_previous_months.nc\")\n",
    "dates = climate_data.time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_climate_shock(from_date, to_date, lat, lon):\n",
    "    if pd.isna(from_date):\n",
    "        return np.nan\n",
    "    \n",
    "    # Filter point    \n",
    "    point_data = climate_data.sel(time=slice(from_date, to_date)).sel(lat=lat, lon=lon, method='nearest')\n",
    "    \n",
    "    # Get position of original data\n",
    "    lat = point_data.lat.item()\n",
    "    lon = point_data.lon.item()\n",
    "\n",
    "    # Filter by time\n",
    "    inutero_q1   = point_data.isel(time=slice(0,3))\n",
    "    inutero_q2   = point_data.isel(time=slice(3,6))\n",
    "    inutero_q3   = point_data.isel(time=slice(6,9))\n",
    "    born_1m      = point_data.isel(time=slice(9,10))\n",
    "    born_2to3m  = point_data.isel(time=slice(10,12))\n",
    "    born_3to6m  = point_data.isel(time=slice(12,15))\n",
    "    born_6to12m  = point_data.isel(time=slice(15,21))\n",
    "\n",
    "    out_vars = [lat, lon, ]\n",
    "    for prec in [\"standarized_precipitation\", \"standarized_precipitation_3\", \"standarized_precipitation_6\", \"standarized_precipitation_12\"]:\n",
    "        # Compute min and max values for both variables\n",
    "        inutero_q1_mean   = inutero_q1[prec].mean().item()\n",
    "        inutero_q2_mean   = inutero_q2[prec].mean().item()\n",
    "        inutero_q3_mean   = inutero_q3[prec].mean().item()\n",
    "        born_1m_mean      = born_1m[prec].mean().item()\n",
    "        born_2to3m_mean  = born_2to3m[prec].mean().item()\n",
    "        born_3to6m_mean  = born_3to6m[prec].mean().item()\n",
    "        born_6to12m_mean  = born_6to12m[prec].mean().item()\n",
    "\n",
    "        out_vars_this_prec = [inutero_q1_mean, inutero_q2_mean, inutero_q3_mean, born_1m_mean, born_2to3m_mean, born_3to6m_mean, born_6to12m_mean]\n",
    "        out_vars += out_vars_this_prec\n",
    "\n",
    "    return out_vars    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_climate_shock_old(from_date, to_date, lat, lon):\n",
    "    if pd.isna(from_date):\n",
    "        return np.nan\n",
    "    \n",
    "    # Filter point    \n",
    "    point_data = climate_data.sel(time=slice(from_date, to_date)).sel(lat=lat, lon=lon, method='nearest')\n",
    "    \n",
    "    # Get max and min values for standarized precipitation\n",
    "    max_prec = point_data[\"standarized_precipitation\"].max().item()\n",
    "    min_prec = point_data[\"standarized_precipitation\"].min().item()\n",
    "    max_prec_m = point_data[\"standarized_precipitation_m\"].max().item()\n",
    "    min_prec_m = point_data[\"standarized_precipitation_m\"].min().item()\n",
    "    \n",
    "    # Get position of original data\n",
    "    lat = point_data.lat.item()\n",
    "    lon = point_data.lon.item()\n",
    "    \n",
    "    return lat, lon, max_prec, min_prec, max_prec_m, min_prec_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_data.isel(time=slice(100, 110), lat=-50, lon=120)[\"standarized_precipitation\"].mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "def get_climate_shock_prof():\n",
    "    date = np.random.choice(dates[12:-12])\n",
    "    from_date, to_date = date + pd.DateOffset(months=-9), date + pd.DateOffset(years=1)\n",
    "    lat, lon = np.random.uniform(-90, 90), np.random.uniform(-180, 180)\n",
    "    \n",
    "    get_climate_shock(from_date, to_date, lat, lon)    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f get_climate_shock_prof get_climate_shock_prof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit get_climate_shock_prof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open DHS data and add the shock data to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_stata(rf\"{DATA_IN}/DHS/DHSBirthsGlobalAnalysis_04172024.dta\")\n",
    "df['ID'] = df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dates variables:\n",
    "We considered a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datetime object from year and month\n",
    "df[\"day\"] = 1\n",
    "df[\"month\"] = df[\"chb_month\"].astype(float)\n",
    "df[\"year\"] = df[\"chb_year\"].astype(float)\n",
    "df[\"birthdate\"] = pd.to_datetime(df[[\"year\", \"month\",\"day\"]]).to_numpy()\n",
    "\n",
    "# Maximum range of dates\n",
    "df[\"from_date\"] = df[\"birthdate\"] + pd.DateOffset(months=-9) # From in utero (9 months before birth) \n",
    "df[\"to_date\"] = df[\"birthdate\"] + pd.DateOffset(years=1) # To the first year of life\n",
    "\n",
    "# Filter children from_date greater than 1990 (we only have climate data from 1990)\n",
    "df = df[df[\"from_date\"] > \"1990-01-01\"]\n",
    "\n",
    "# # Construct deathdate variable\n",
    "# df[\"deathdate\"] = df[df[\"child_agedeath\"]<=12].progress_apply(lambda x: x[\"birthdate\"] + pd.DateOffset(months=x[\"child_agedeath\"]), axis=1)\n",
    "\n",
    "# # Replace to_date with deathdate if the child died\n",
    "# df[\"to_date\"] = np.where((df[\"child_agedeath\"]<=12) & (df[\"deathdate\"]<df[\"to_date\"]), df[\"deathdate\"], df[\"to_date\"])\n",
    "\n",
    "# Filter children to_date smalle than 2021 (we only have climate data to 2020)\n",
    "df = df[df[\"to_date\"] < \"2021-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_cols = [\"lat_climate\", \"lon_climate\"]\n",
    "prec_cols = [\"prec_inutero_q1\", \"prec_inutero_q2\", \"prec_inutero_q3\", \"prec_born_1m\", \"prec_born_2to3m\", \"prec_born_3to6m\", \"prec_born_6to12m\"]\n",
    "prec_3_cols = [\"prec_3_inutero_q1\", \"prec_3_inutero_q2\", \"prec_3_inutero_q3\", \"prec_3_born_1m\", \"prec_3_born_2to3m\", \"prec_3_born_3to6m\", \"prec_3_born_6to12m\"]\n",
    "prec_6_cols = [\"prec_6_inutero_q1\", \"prec_6_inutero_q2\", \"prec_6_inutero_q3\", \"prec_6_born_1m\", \"prec_6_born_2to3m\", \"prec_6_born_3to6m\", \"prec_6_born_6to12m\"]\n",
    "prec_12_cols = [\"prec_12_inutero_q1\", \"prec_12_inutero_q2\", \"prec_12_inutero_q3\", \"prec_12_born_1m\", \"prec_12_born_2to3m\", \"prec_12_born_3to6m\", \"prec_12_born_6to12m\"]\n",
    "all_cols = coords_cols + prec_cols + prec_3_cols + prec_6_cols + prec_12_cols\n",
    "\n",
    "for n in tqdm(range(0, df.ID.max(), 10_000)):\n",
    "    if os.path.exists(rf\"{DATA_PROC}/births_climate_{n}.csv\"):\n",
    "        print(f\"births_climate_{n}.csv exists, moving to next iteration\")\n",
    "        continue\n",
    "    chunk = df.loc[(df.ID >= n) & (df.ID < n+10_000), ['ID', 'from_date', 'to_date', 'LATNUM', 'LONGNUM']].copy()\n",
    "    if chunk.shape[0]==0:\n",
    "        continue\n",
    "    climate_results = chunk[['from_date', 'to_date', 'LATNUM', 'LONGNUM']].apply(lambda s: get_climate_shock(s['from_date'], s['to_date'], s['LATNUM'], s['LONGNUM']), axis=1)\n",
    "    climate_results = climate_results.apply(pd.Series)\n",
    "    climate_results.columns = all_cols\n",
    "    climate_results[\"ID\"] = chunk[\"ID\"]\n",
    "    climate_results.to_csv(rf\"{DATA_PROC}/births_climate_{n}.csv\")\n",
    "    \n",
    "# df[all_cols] = climate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For debugging\n",
    "# chunk[['from_date', 'to_date', 'LATNUM', 'LONGNUM']].progress_apply(lambda s: get_climate_shock(s['from_date'], s['to_date'], s['LATNUM'], s['LONGNUM']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(rf\"{DATA_PROC}\") \n",
    "files = [f for f in files if f.startswith(\"births_climate_\")]\n",
    "data = []\n",
    "for file in tqdm(files):\n",
    "    df = pd.read_csv(rf\"{DATA_PROC}/{file}\")\n",
    "    data += [df]\n",
    "df = pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=\"Unnamed: 0\")\n",
    "df.to_stata(rf\"{DATA_PROC}\\ClimateShocks_assigned.dta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pos = np.random.randint(0, 500000)\n",
    "pos = 428380      \n",
    "filtered2 = climate_data.standarized_precipitation.sel(lat=df.at[pos, \"LATNUM\"], lon=df.at[pos, \"LONGNUM\"], method=\"nearest\")\n",
    "filtered2.plot(figsize=(12, 2))\n",
    "\n",
    "plt.axhline(2, color=\"red\")\n",
    "plt.axhline(-2, color=\"red\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "climate = pd.read_stata(rf\"{DATA_PROC}\\ClimateShocks_assigned.dta\")\n",
    "\n",
    "dhs = pd.read_stata(rf\"{DATA_IN}/DHS/DHSBirthsGlobalAnalysis_04172024.dta\")\n",
    "dhs[\"ID\"] = dhs.index\n",
    "\n",
    "merged = dhs.merge(climate, on=\"ID\")\n",
    "dhs = 0\n",
    "climate = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged = merged.head(10_000)\n",
    "merged[\"lon_climate_2\"] = merged[\"lon_climate\"].round(0) \n",
    "merged[\"lon_climate_3\"] = merged[\"lon_climate_2\"] - merged[\"lon_climate_2\"].astype(int) % 2 \n",
    "merged[\"lat_climate_2\"] = merged[\"lat_climate\"].round(0)\n",
    "merged[\"lat_climate_3\"] = merged[\"lat_climate_2\"] - merged[\"lat_climate_2\"].astype(int) % 2 \n",
    "\n",
    "merged[\"ID_cell3\"] = merged[\"lon_climate_3\"].astype(str) + \"_\" + merged[\"lat_climate_3\"].astype(str)\n",
    "one_hot = pd.get_dummies(merged[\"ID_cell3\"], prefix='ID_cell3')\n",
    "years_interaction = one_hot.multiply(merged[\"chb_year\"], axis=\"index\")\n",
    "years_interaction.columns = [f\"years_{col}\" for col in years_interaction.columns]\n",
    "months_interaction = one_hot.multiply(merged[\"chb_year\"], axis=\"index\")\n",
    "months_interaction.columns = [f\"months_{col}\" for col in months_interaction.columns]\n",
    "gc.collect()\n",
    "\n",
    "merged[one_hot.columns] = one_hot\n",
    "merged[years_interaction.columns] = years_interaction\n",
    "merged[months_interaction.columns] = months_interaction\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[one_hot.columns] = one_hot\n",
    "merged[years_interaction.columns] = years_interaction\n",
    "merged[months_interaction.columns] = months_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([merged, one_hot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot.memory_usage().sum() * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ofici\\AppData\\Local\\Temp\\ipykernel_6032\\669018010.py:2: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  df = pd.read_stata(rf\"{DATA_OUT}\\DHSBirthsGlobal&ClimateShocks.dta\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_stata(rf\"{DATA_OUT}\\DHSBirthsGlobal&ClimateShocks.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastparquet\n",
      "  Obtaining dependency information for fastparquet from https://files.pythonhosted.org/packages/2d/58/c579cbdfa257e93f9f6c04a6ec620a42dcd361d70dbc09325a61b7d018b0/fastparquet-2024.2.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading fastparquet-2024.2.0-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\ofici\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastparquet) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\ofici\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastparquet) (1.24.3)\n",
      "Collecting cramjam>=2.3 (from fastparquet)\n",
      "  Obtaining dependency information for cramjam>=2.3 from https://files.pythonhosted.org/packages/08/32/1f90bee4b86d1b92fb76c26f11db07b5bce7db842fb1cc912ed7f045b696/cramjam-2.8.3-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading cramjam-2.8.3-cp311-none-win_amd64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ofici\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastparquet) (2023.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ofici\\appdata\\roaming\\python\\python311\\site-packages (from fastparquet) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ofici\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ofici\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ofici\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ofici\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Downloading fastparquet-2024.2.0-cp311-cp311-win_amd64.whl (670 kB)\n",
      "   ---------------------------------------- 0.0/670.5 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 286.7/670.5 kB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  665.6/670.5 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 670.5/670.5 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading cramjam-2.8.3-cp311-none-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.6/1.6 MB 11.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.1/1.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 9.5 MB/s eta 0:00:00\n",
      "Installing collected packages: cramjam, fastparquet\n",
      "Successfully installed cramjam-2.8.3 fastparquet-2024.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!py -m pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(rf\"{DATA_OUT}\\DHSBirthsGlobal&ClimateShocks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.dtypes == \"Categorical\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
